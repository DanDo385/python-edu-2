# Solution Explanation: Fine-tuning LLMs

## Overview

LoRA enables parameter-efficient fine-tuning by adding low-rank matrices to pretrained weights.

## Key Points

- Freeze original weights
- Add trainable low-rank matrices
- Much fewer parameters than full fine-tuning
- Effective for adapting LLMs
