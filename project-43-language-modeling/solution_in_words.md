# Solution Explanation: Language Modeling

## Overview

Language modeling predicts the next token. Decoder-only transformers with causal masking are used (GPT-style).

## Key Points

- Causal masking prevents seeing future tokens
- Autoregressive generation produces text token by token
- Foundation for GPT and other LLMs
