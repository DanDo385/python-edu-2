# Project 43: Language Modeling with Transformers

## Learning Objectives

- Understand language modeling (predicting next token)
- Build GPT-like decoder-only transformer
- Implement causal (masked) self-attention
- Train language model on text data
- Generate text from trained model

## Problem Description

Language modeling predicts the next token in a sequence. GPT models use decoder-only transformers with causal masking.

## Key Concepts

- Causal masking (prevent seeing future tokens)
- Autoregressive generation
- Next token prediction
- Text generation

## Deliverables

Decoder-only transformer for language modeling with training and generation.

## Testing

Run: `pytest test.py -v`
